<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Optimal Control and Reinforcement Learning - GAMES105</title>
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="The note of GAMES105">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/pagetoc.css">
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><div>GAMES105</div></li><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="Introduction.html"><strong aria-hidden="true">2.</strong> Introduction to 3D Character Animation</a></li><li class="chapter-item expanded "><a href="Math.html"><strong aria-hidden="true">3.</strong> Math Background</a></li><li class="chapter-item expanded "><a href="Forward.html"><strong aria-hidden="true">4.</strong> Character Kinematics:Forward and Inverse Kinematics</a></li><li class="chapter-item expanded "><a href="Keyframe.html"><strong aria-hidden="true">5.</strong> Character Kinematics (cont.) &amp; Keyframe Animation</a></li><li class="chapter-item expanded "><a href="Data-driven.html"><strong aria-hidden="true">6.</strong> Data-driven Character Animation</a></li><li class="chapter-item expanded "><a href="Learning-based.html"><strong aria-hidden="true">7.</strong> Statistical Models of Human Motion</a></li><li class="chapter-item expanded "><a href="cont.html"><strong aria-hidden="true">8.</strong> Learning-based Character Animation</a></li><li class="chapter-item expanded "><a href="Skinning.html"><strong aria-hidden="true">9.</strong> Skinning</a></li><li class="chapter-item expanded "><a href="Simulation.html"><strong aria-hidden="true">10.</strong> Physics-based Simulation and Articulated Rigid Bodies</a></li><li class="chapter-item expanded "><a href="Actuating.html"><strong aria-hidden="true">11.</strong> Actuating Simulated Characters</a></li><li class="chapter-item expanded "><a href="Controlling.html"><strong aria-hidden="true">12.</strong> Controlling Characters</a></li><li class="chapter-item expanded "><a href="Learning.html"><strong aria-hidden="true">13.</strong> Learning to Walk</a></li><li class="chapter-item expanded "><a href="Optimal.html" class="active"><strong aria-hidden="true">14.</strong> Optimal Control and Reinforcement Learning</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">GAMES105</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/CaterpillarStudyGroup/GAMES105_mdbook" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main><div class="sidetoc"><nav class="pagetoc"></nav></div>
                        <p>P2</p>
<h1 id="outline"><a class="header" href="#outline">Outline</a></h1>
<ul>
<li>
<p>Optimal Control</p>
</li>
<li>
<p>Model-based Approaches vs. Model-free Approaches</p>
</li>
<li>
<p>Sampling-based Optimization</p>
</li>
<li>
<p>Reinforcement Learning</p>
</li>
<li>
<p>Conclusion</p>
</li>
</ul>
<p>P3</p>
<h2 id="recap"><a class="header" href="#recap">Recap</a></h2>
<table><thead><tr><th>feedforward</th><th>feedback</th></tr></thead><tbody>
<tr><td><img src="./assets/12-01.png" alt="" /></td><td><img src="./assets/12-04.png" alt="" /></td></tr>
<tr><td><img src="./assets/12-02.png" alt="" /></td><td><img src="./assets/12-03.png" alt="" /></td></tr>
</tbody></table>
<blockquote>
<p>âœ… å¼€ç¯æ§åˆ¶ï¼šåªè€ƒè™‘åˆå§‹çŠ¶æ€ã€‚<br />
âœ… å‰é¦ˆæ§åˆ¶ï¼šè€ƒè™‘åˆå§‹çŠ¶æ€å’Œå¹²æŒ ã€‚<br />
âœ… å‰é¦ˆæ§åˆ¶ä¼˜åŒ–çš„æ˜¯è½¨è¿¹ã€‚<br />
âœ… åé¦ˆæ§åˆ¶ä¼˜åŒ–çš„æ˜¯æ§åˆ¶ç­–ç•¥ï¼Œæ§åˆ¶ç­–ç•¥æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®å½“å‰çŠ¶æ€ä¼˜åŒ–è½¨è¿¹ã€‚</p>
</blockquote>
<p>P9</p>
<p><img src="./assets/12-05.png" alt="" /></p>
<blockquote>
<p>âœ… Feedback ç±»ä¼¼æ„é€ ä¸€ä¸ªåœºï¼ŒæŠŠä»»ä½•çŠ¶æ€æ¨åˆ°ç›®æ ‡çŠ¶æ€ã€‚</p>
</blockquote>
<p>P10</p>
<h1 id="å¼€ç¯æ§åˆ¶"><a class="header" href="#å¼€ç¯æ§åˆ¶">å¼€ç¯æ§åˆ¶</a></h1>
<h2 id="é—®é¢˜æè¿°"><a class="header" href="#é—®é¢˜æè¿°">é—®é¢˜æè¿°</a></h2>
<p>$$
\begin{matrix}
\min_{x}  f(x)\\
ğ‘ .ğ‘¡. g(x)=0
\end{matrix}
$$</p>
<p><img src="./assets/12-06.png" alt="" /></p>
<p>P12</p>
<h2 id="æŠŠç¡¬çº¦æŸè½¬åŒ–ä¸ºè½¯çº¦æŸ"><a class="header" href="#æŠŠç¡¬çº¦æŸè½¬åŒ–ä¸ºè½¯çº¦æŸ">æŠŠç¡¬çº¦æŸè½¬åŒ–ä¸ºè½¯çº¦æŸ</a></h2>
<p>$$
\min_{x}  f(x)+ wg(x)
$$</p>
<p>\(^\ast \) The solution \(x^\ast\)  may not satisfy the constraint</p>
<p>P16</p>
<h2 id="lagrange-multiplier---æŠŠçº¦æŸæ¡ä»¶è½¬åŒ–ä¸ºä¼˜åŒ–"><a class="header" href="#lagrange-multiplier---æŠŠçº¦æŸæ¡ä»¶è½¬åŒ–ä¸ºä¼˜åŒ–">Lagrange Multiplier - æŠŠçº¦æŸæ¡ä»¶è½¬åŒ–ä¸ºä¼˜åŒ–</a></h2>
<blockquote>
<p>âœ… æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•ã€‚</p>
</blockquote>
<p><img src="./assets/12-08.png" alt="" /></p>
<blockquote>
<p>âœ… é€šè¿‡è§‚å¯Ÿå¯çŸ¥ï¼Œæå€¼ç‚¹ä½äº\({f}'(x)\) ä¸ \(g\) çš„åˆ‡çº¿å‚ç›´ï¼Œå³ \({f}' (x)\) ä¸ \({g}' (x)\) å¹³è¡Œã€‚ï¼ˆå……åˆ†éå¿…è¦æ¡ä»¶ã€‚ï¼‰</p>
</blockquote>
<p>å› æ­¤ï¼š</p>
<p><img src="./assets/12-07.png" alt="" /></p>
<p>Lagrange function</p>
<p>$$
L(x,\lambda )=f(x)+\lambda ^Tg(x)
$$</p>
<blockquote>
<p>âœ… æŠŠçº¦æŸæ¡ä»¶è½¬åŒ–ä¸ºä¼˜åŒ–ã€‚</p>
</blockquote>
<p>P18</p>
<h2 id="lagrange-multiplier"><a class="header" href="#lagrange-multiplier">Lagrange Multiplier</a></h2>
<p><img src="./assets/12-09-1.png" alt="" /></p>
<blockquote>
<p>âœ… è¿™æ˜¯ä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™æ‰¾åˆ°æå€¼ç‚¹ã€‚</p>
</blockquote>
<p>P20</p>
<h2 id="solving-trajectory-optimization-problem"><a class="header" href="#solving-trajectory-optimization-problem">Solving Trajectory Optimization Problem</a></h2>
<h3 id="å®šä¹‰å¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜"><a class="header" href="#å®šä¹‰å¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜">å®šä¹‰å¸¦çº¦æŸçš„ä¼˜åŒ–é—®é¢˜</a></h3>
<p>Find a control sequence {\(a_t\)} that generates a state sequence {\(s_t\)} start from \(s_o\) minimizes</p>
<p>$$
\min h (s_r)+\sum _{t=0}^{T-1} h(s_t,a_t)
$$</p>
<blockquote>
<p>âœ… å› ä¸ºæŠŠæ—¶é—´ç¦»æ•£åŒ–ï¼Œæ­¤å¤„ç”¨æ±‚å’Œä¸ç”¨ç§¯åˆ†ã€‚</p>
</blockquote>
<p>subject to</p>
<p>$$
\begin{matrix}
f(s_t,a_t)-s_{t+1}=0\\
\text{ for } 0 \le t &lt; T
\end{matrix}
$$</p>
<blockquote>
<p>âœ… è¿åŠ¨å­¦æ–¹ç¨‹ï¼Œä½œä¸ºçº¦æŸ</p>
</blockquote>
<h3 id="è½¬åŒ–ä¸ºä¼˜åŒ–é—®é¢˜"><a class="header" href="#è½¬åŒ–ä¸ºä¼˜åŒ–é—®é¢˜">è½¬åŒ–ä¸ºä¼˜åŒ–é—®é¢˜</a></h3>
<p>The Lagrange function</p>
<p>$$
L(s,a,\lambda ) = h(s _ T)+ \sum _ {t=0} ^ {T-1} h(s _t,a _t) + \lambda _ {t+1}^T(f(s _t,a _t) - s _ {t+1})
$$</p>
<p>P27</p>
<h3 id="æ±‚è§£æ‹‰æ ¼æœ—æ—¥æ–¹ç¨‹"><a class="header" href="#æ±‚è§£æ‹‰æ ¼æœ—æ—¥æ–¹ç¨‹">æ±‚è§£æ‹‰æ ¼æœ—æ—¥æ–¹ç¨‹</a></h3>
<p><img src="./assets/12-10-1.png" alt="" /></p>
<blockquote>
<p>âœ… æ‹‰æ ¼æœ—æ—¥æ–¹ç¨‹ï¼Œå¯¹æ¯ä¸ªå˜é‡æ±‚å¯¼ï¼Œå¹¶ä»¤å¯¼æ•°ä¸ºé›¶ã€‚å› æ­¤å¾—åˆ°å³è¾¹æ–¹ç¨‹ç»„ã€‚<br />
âœ… å³è¾¹æ–¹ç¨‹ç»„è¿›ä¸€æ­¥æ•´ç†ï¼Œå¾—åˆ°å·¦è¾¹ã€‚<br />
âœ… \(\lambda \) ç±»ä¼¼äºé€†å‘ä»¿çœŸã€‚<br />
âœ… å…¬å¼ 3ï¼šé€šè¿‡è½¬ä¸ºä¼˜åŒ–é—®é¢˜æ±‚ \(a\)ï¼</p>
</blockquote>
<p>P30</p>
<h3 id="pontryagins-maximum-principle-for-discrete-systems"><a class="header" href="#pontryagins-maximum-principle-for-discrete-systems">Pontryaginâ€™s Maximum Principle for discrete systems</a></h3>
<p><img src="./assets/12-11.png" alt="" /></p>
<p><img src="./assets/12-12.png" alt="" /></p>
<blockquote>
<p>âœ… æ–¹ç¨‹ç»„æ•´ç†å¾—åˆ°å·¦è¾¹ï¼Œç§°ä¸º PMP æ¡ä»¶ã€‚æ˜¯å¼€ç¯æ§åˆ¶æœ€ä¼˜çš„å¿…è¦æ¡ä»¶ã€‚</p>
</blockquote>
<p>P32</p>
<h2 id="optimal-control"><a class="header" href="#optimal-control">Optimal Control</a></h2>
<p><strong>Open-loop Control</strong>:<br />
given a start state \(s_0\), compute sequence of actions {\(a_t\)} to reach the goal</p>
<p><img src="./assets/12-13.png" alt="" /></p>
<blockquote>
<p><strong>Shooting method</strong> directly applies PMP. However, it does not scale well to complicated problems such as motion controlâ€¦<br />
\(<br>\)<br />
Need to be combined with collocation method, multiple shooting, etc. for those problems.<br />
\(<br>\)<br />
Or use derivative-free approaches.</p>
</blockquote>
<p><img src="./assets/12-14.png" alt="" /></p>
<blockquote>
<p>âœ… å¯¹äºå¤æ‚å‡½æ•°ï¼Œè¡¨ç°æ¯”è¾ƒå·®ï¼Œè¿˜éœ€è¦å€ŸåŠ©å…¶å®ƒæ–¹æ³•ã€‚</p>
</blockquote>
<h1 id="é—­ç¯æ§åˆ¶"><a class="header" href="#é—­ç¯æ§åˆ¶">é—­ç¯æ§åˆ¶</a></h1>
<p><img src="./assets/12-05.png" alt="" /></p>
<p>P34</p>
<h2 id="dynamic-programming"><a class="header" href="#dynamic-programming">Dynamic Programming</a></h2>
<p><img src="./assets/12-15.png" alt="" /> </p>
<p>å¸Œæœ›æ‰¾åˆ°ä¸€æ¡æœ€çŸ­è·¯å¾„åˆ°è¾¾å¦ä¸€ä¸ªç‚¹ï¼Œå¯¹è¿™ä¸ªé—®é¢˜ç”¨ä¸åŒçš„æ–¹å¼å»ºæ¨¡ï¼Œä¼šå¾—åˆ°ä¸åŒçš„æ–¹æ³•ï¼š</p>
<table><thead><tr><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>åŠ¨æ€è§„åˆ’é—®é¢˜</td><td>Find a path {\(s_t\)} that minimizes</td><td>\(J(s_0)=\sum _ {t=0}^{ } h(s_t,s_{t+1})\)</td></tr>
<tr><td>è½¨è¿¹é—®é¢˜</td><td>Find a sequence of action {\(a_t\)} that minimizes</td><td>\(J(s_0)=\sum _ {t=0}^{ } h(s_t,a_t)\)<br> subject to <br> \( s_{t+1}=f(s_t,a_t)\)</td></tr>
<tr><td>æ§åˆ¶ç­–ç•¥é—®é¢˜</td><td>Find a policy \( a_t=\pi (s_t,t)\)æˆ– \( a_t=\pi (s_t)\)that minimizes</td><td>\(J(s_0)=\sum _ {t=0}^{ } h(s_t,a_t)\)<br>subject to  <br>\(s_{t+1}=f(s_t,a_t)\)</td></tr>
</tbody></table>
<p>P39</p>
<h2 id="bellmans-principle-of-optimality"><a class="header" href="#bellmans-principle-of-optimality">Bellmanâ€™s Principle of Optimality</a></h2>
<blockquote>
<p>âœ… é’ˆå¯¹æ§åˆ¶ç­–ç•¥é—®é¢˜ï¼Œä»€ä¹ˆæ ·çš„ç­–ç•¥æ˜¯æœ€ä¼˜ç­–ç•¥ï¼Ÿ</p>
</blockquote>
<p><img src="./assets/12-16.png" alt="" /> </p>
<p>An optimal policy has the property that whatever the initial 
state and initial decision are, the remaining decisions must 
constitute an optimal policy with regard to the state resulting 
from the first decision.</p>
<p>\(^\ast \) The problem is said to have <strong>optimal substructure</strong></p>
<p>P40</p>
<h2 id="value-function"><a class="header" href="#value-function">Value Function</a></h2>
<p>Value of a state \(V(s)\) :</p>
<ul>
<li>the minimal total cost for finishing the task starting from \(s\)</li>
<li>the total cost for finishing the task starting from \(s\) using the optimal policy</li>
</ul>
<blockquote>
<p>âœ… Value Funcronï¼Œè®¡ç®—ä»æŸä¸ªç»“ç‚¹åˆ° gool çš„æœ€å°ä»£ä»·ã€‚<br />
âœ… åé¢åŠ¨æ€è§„åˆ’åŸç†è·³è¿‡ã€‚</p>
</blockquote>
<p>P49</p>
<h2 id="the-bellman-equation"><a class="header" href="#the-bellman-equation">The Bellman Equation</a></h2>
<p>Mathematically, an optimal <strong>value function</strong> \(V(s)\) can be defined recursively as:</p>
<p>$$
V(s)=\min_{a} (h(s,a)+V(f(s,a)))
$$</p>
<blockquote>
<p>âœ… hä»£è¡¨sçŠ¶æ€ä¸‹æ‰§è¡Œä¸€æ­¥açš„ä»£ä»·ã€‚fä»£è¡¨ä»¥sçŠ¶æ€ä¸‹æ‰§è¡Œä¸€æ­¥aä¹‹åçš„çŠ¶æ€ã€‚</p>
</blockquote>
<p>If we know this value function, the optimal <strong>policy</strong> can be computed as</p>
<p>$$
\pi (s)=\arg \min_{a} (h(s,a)+V(f(s,a)))
$$</p>
<blockquote>
<p>âœ… piä»£è¡¨ä¸€ç§ç­–ç•¥ï¼Œæ ¹æ®å½“å‰çŠ¶æ€sæ‰¾åˆ°æœ€ä¼˜çš„ä¸‹ä¸€æ­¥aã€‚<br />
âœ… This arg max can be easily computed for discrete control problems.<br />
But there are not always closed-forms solution for continuous control problems.</p>
</blockquote>
<p>or</p>
<p>$$
\begin{matrix}
\pi (s)=\arg \min_{a} Q(s,a)\\
\text{where} \quad \quad  Q(s,a)=h(s,a)+V(f(s,a))
\end{matrix}
$$</p>
<p>Q-functionç§°ä¸ºState-action value function<br />
Learning \(V(s)\) and/or \(Q(s,a)\) is the core of optimal control / reinforcement learning methods</p>
<blockquote>
<p>âœ… å¼ºåŒ–å­¦ä¹ æœ€ä¸»è¦çš„ç›®çš„æ˜¯å­¦ä¹  \(V\) å‡½æ•°å’Œ \(Q\) å‡½æ•°ï¼Œå¦‚æœ \(a\) æ˜¯æœ‰é™çŠ¶æ€ï¼Œéå†å³å¯ã€‚ä½†åœ¨è§’è‰²åŠ¨ç”»é‡Œï¼Œ\(a\) æ˜¯è¿ç»­çŠ¶æ€ã€‚</p>
</blockquote>
<p>P52</p>
<h1 id="linear-quadratic-regulator-lqr"><a class="header" href="#linear-quadratic-regulator-lqr">Linear Quadratic Regulator (LQR)</a></h1>
<p><img src="./assets/12-17.png" alt="" /> </p>
<ul>
<li>LQR is a special class of optimal control problems with
<ul>
<li><strong>Linear</strong> dynamic function</li>
<li><strong>Quadratic</strong> objective function</li>
</ul>
</li>
</ul>
<blockquote>
<p>âœ… LQR æ˜¯æ§åˆ¶é¢†åŸŸä¸€ç±»ç»å…¸é—®é¢˜ï¼Œå®ƒå¯¹åŸæ§åˆ¶é—®é¢˜åšäº†ä¸€äº›ç‰¹å®šçš„çº¦æŸã€‚å› ä¸ºç®€åŒ–äº†é—®é¢˜ï¼Œå¯ä»¥å¾—åˆ°æœ‰ç‰¹å®šå…¬å¼çš„ \(Q\) å’Œ \(V\).</p>
</blockquote>
<p>P53</p>
<h2 id="a-very-simple-example"><a class="header" href="#a-very-simple-example">A very simple example</a></h2>
<h3 id="é—®é¢˜æè¿°-1"><a class="header" href="#é—®é¢˜æè¿°-1">é—®é¢˜æè¿°</a></h3>
<p><img src="./assets/12-18.png" alt="" /> </p>
<p>Compute a target trajectory \(\tilde{x}(t)\) such that the simulated trajectory \(x(t)\) is a sine curve.</p>
<p><img src="./assets/12-19.png" alt="" /> </p>
<p>$$
\min _{(x_n,v_n,\tilde{x} _n)} \sum _{n=0}^{N} (\sin (t_n)-x_n)^2+\sum _{n=0}^{N}\tilde{x}^2_n 
$$</p>
<p>$$
\begin{align*}
s.t. \quad \quad v _ {n+1} &amp; = v _ n + h(k _p ( \tilde{x} _ n - x _ n) - k _ dv _ n ) \\
v _ {x+1} &amp; = x _ n + hv _ {n+1}
\end{align*}
$$</p>
<p>P54<br />
objective function</p>
<p>$$
\min s^T_TQ_Ts_T+\sum_{t=0}^{T} s^T_tQ_ts_t+a^T_tR_ta_t
$$</p>
<p>subject to dynamic function</p>
<p>$$
s_{t+1}=A_ts_t+B_ta_t   \quad \quad \text{for }   0\le t &lt;T 
$$</p>
<blockquote>
<p>âœ… ç›®æ ‡å‡½æ•°æ˜¯äºŒæ¬¡å‡½æ•°ï¼Œè¿åŠ¨å­¦æ–¹ç¨‹æ˜¯çº¿æ€§å‡½æ•°ã€‚è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„ LQR é—®é¢˜ã€‚</p>
</blockquote>
<p>P58</p>
<h3 id="æ¨å¯¼ä¸€æ­¥"><a class="header" href="#æ¨å¯¼ä¸€æ­¥">æ¨å¯¼ä¸€æ­¥</a></h3>
<blockquote>
<p>âœ… ç”±äºå­˜åœ¨optimal substructureï¼Œæ¯æ¬¡åªéœ€è¦è€ƒè™‘ä¸‹ä¸€ä¸ªçŠ¶æ€çš„æœ€ä¼˜è§£ã€‚<br />
âœ… æ¯ä¸€ä¸ªçŠ¶æ€åŸºäºä¸‹ä¸€ä¸ªçŠ¶æ€æ¥è®¡ç®—ï¼Œä¸æ–­å¾€ä¸‹è¿­ä»£ï¼Œç›´åˆ°æœ€åä¸€ä¸ªçŠ¶æ€ã€‚<br />
âœ… æœ€åä¸€ä¸ªçŠ¶æ€çš„Vçš„è®¡ç®—ä¸aæ— å…³ã€‚<br />
âœ… è®¡ç®—å®Œæœ€åä¸€ä¸ªï¼Œå†è®¡ç®—å€’æ•°ç¬¬äºŒä¸ªï¼Œä¾æ¬¡å¾€å‰æ¨ã€‚</p>
</blockquote>
<p><img src="./assets/12-20-1.png" alt="" /> </p>
<p>P60<br />
å…¬å¼æ•´ç†å¾—ï¼š</p>
<p><img src="./assets/12-20.png" alt="" /> </p>
<p>P61 
<img src="./assets/12-21.png" alt="" /> </p>
<blockquote>
<p>âœ… ç»“è®ºï¼šæœ€ä¼˜ç­–ç•¥ä¸å½“å‰çŠ¶æ€çš„å…³ç³»æ˜¯çŸ©é˜µKçš„å…³ç³»ã€‚</p>
</blockquote>
<p>P62<br />
å½“aå–æœ€å°å€¼æ—¶ï¼Œæ±‚å‡ºVï¼š</p>
<p><img src="./assets/12-22.png" alt="" /></p>
<blockquote>
<p>âœ… \(V(S_{T-1})\)å’Œ\(V(S_{T})\)çš„å½¢å¼åŸºæœ¬ä¸€è‡´ï¼Œåªæ˜¯Pçš„è¡¨ç¤ºä¸åŒã€‚ </p>
</blockquote>
<p>P63</p>
<h3 id="æ¨å¯¼æ¯ä¸€æ­¥"><a class="header" href="#æ¨å¯¼æ¯ä¸€æ­¥">æ¨å¯¼æ¯ä¸€æ­¥</a></h3>
<p><img src="./assets/12-23.png" alt="" /> </p>
<p>P64</p>
<h3 id="solution"><a class="header" href="#solution">Solution</a></h3>
<ul>
<li>LQR is a special class of optimal control problems with
<ul>
<li>Linear dynamic function</li>
<li>Quadratic objective function</li>
</ul>
</li>
<li>Solution of LQR is a linear feedback policy</li>
</ul>
<p><img src="./assets/12-24.png" alt="" /> </p>
<p>P65</p>
<h2 id="æ›´å¤æ‚çš„æƒ…å†µ"><a class="header" href="#æ›´å¤æ‚çš„æƒ…å†µ">æ›´å¤æ‚çš„æƒ…å†µ</a></h2>
<ul>
<li>How to deal with
<ul>
<li>Nonlinear dynamic function?</li>
<li>Non-quadratic objective function?</li>
</ul>
</li>
</ul>
<blockquote>
<p>âœ… äººä½“è¿åŠ¨æ¶‰åŠåˆ°è§’åº¦æ—‹è½¬ï¼Œå› æ­¤æ˜¯éçº¿æ€§çš„ã€‚</p>
</blockquote>
<p>P68</p>
<h3 id="nonlinear-problems"><a class="header" href="#nonlinear-problems">Nonlinear problems</a></h3>
<p><img src="./assets/12-25.png" alt="" /> </p>
<blockquote>
<p>âœ… æ–¹æ³•ï¼šæŠŠé—®é¢˜è¿‘ä¼¼ä¸ºçº¿æ€§é—®é¢˜ã€‚</p>
</blockquote>
<p>Approximate cost function as a quadratic function:</p>
<blockquote>
<p>âœ… ç›®æ ‡å‡½æ•°ï¼šæ³°å‹’å±•å¼€ï¼Œä¿ç•™äºŒæ¬¡ã€‚</p>
</blockquote>
<p>$$
h(s_t,a_t)\approx h(\bar{s}_t ,\bar{a}_t)+\nabla h(\bar{s}_t ,\bar{a}_t)\begin{bmatrix}
s_t-\bar{s} _t\\
a_t-\bar{a} _t
\end{bmatrix} + \frac{1}{2} \begin{bmatrix}
s_t-\bar{s} _t\\
a_t-\bar{a} _t
\end{bmatrix}^T\nabla^2h(\bar{s}_t ,\bar{a}_t)\begin{bmatrix}
s_t-\bar{s} _t\\
a_t-\bar{a} _t
\end{bmatrix}
$$</p>
<p>Approximate dynamic function as a linear function:</p>
<blockquote>
<p>âœ… è½¬ç§»å‡½æ•°ï¼šæ³°å‹’å±•å¼€ï¼Œä¿ç•™ä¸€æ¬¡æˆ–äºŒæ¬¡ã€‚</p>
</blockquote>
<p>$$
f(s_t,a_t)\approx f(\bar{s}_t ,\bar{a}_t)+\nabla f(\bar{s}_t ,\bar{a}_t)\begin{bmatrix}
s_t-\bar{s} _t\\
a_t-\bar{a} _t
\end{bmatrix}
$$</p>
<p>å±•å¼€ä¸ºä¸€æ¬¡é¡¹ï¼Œå¯¹åº”è§£å†³ç®—æ³•ï¼šiLQRï¼ˆiterative LQRï¼‰ </p>
<p>Or a quadratic function:</p>
<p>$$
f(s_t,a_t)\approx \ast \ast \ast \frac{1}{2} \begin{bmatrix}
s_t-\bar{s} _t\\
a_t-\bar{a} _t
\end{bmatrix}^T\nabla^2f(\bar{s}_t ,\bar{a}_t)\begin{bmatrix}
s_t-\bar{s} _t\\
a_t-\bar{a} _t
\end{bmatrix}
$$</p>
<p>å±•å¼€ä¸ºäºŒæ¬¡é¡¹ï¼Œå¯¹åº”è§£å†³ç®—æ³•ï¼šDDPï¼ˆDifferential Dynamic Programmingï¼‰</p>
<p>P69</p>
<h3 id="ç›¸å…³åº”ç”¨"><a class="header" href="#ç›¸å…³åº”ç”¨">ç›¸å…³åº”ç”¨</a></h3>
<blockquote>
<p>ğŸ” [Muico et al 2011 - Composite Control of Physically Simulated Characters]</p>
</blockquote>
<blockquote>
<p>âœ… é€‰æ‹©åˆé€‚çš„ \(Q\) å’Œ \(R\)ï¼Œéœ€è¦ä¸€äº›å·¥ç¨‹ä¸Šçš„æŠ€å·§ã€‚<br />
âœ… ä¸ºäº†æ±‚è§£æ–¹ç¨‹ï¼Œéœ€è¦æ˜¾å¼åœ°å»ºæ¨¡è¿åŠ¨å­¦æ–¹ç¨‹ã€‚</p>
</blockquote>
<p>P70</p>
<h2 id="model-based-method-vs-model-free-method"><a class="header" href="#model-based-method-vs-model-free-method">Model-based Method vs. Model-free Method</a></h2>
<blockquote>
<p>âœ… Model Based æ–¹æ³•ï¼Œè¦æ±‚ dynamic function æ˜¯å·²çŸ¥çš„ï¼Œä½†æ˜¯å®é™…ä¸Šè¿™ä¸ªå‡½æ•°å¯èƒ½æ˜¯ï¼ˆ1ï¼‰æœªçŸ¥çš„ï¼ˆ2ï¼‰ä¸ç²¾ç¡®çš„ã€‚<br />
âœ… å› æ­¤Model Based æ–¹æ³•å¯¹äºå¤æ‚é—®é¢˜éš¾ä»¥åº”ç”¨ï¼Œä½†å¯¹äºç®€å•é—®é¢˜éå¸¸é«˜æ•ˆã€‚</p>
</blockquote>
<p>What if the dynamic function \(f(s,a)\) is not know?</p>
<blockquote>
<p>âœ… \(f\) æœªçŸ¥åªæ˜¯æŠŠ \(f\) å½“æˆä¸€ä¸ªé»‘ç›’å­ï¼Œä»éœ€è¦æ ¹æ® \(S_t\) å¾—åˆ° \(S_{tï¼‹1}\) .</p>
</blockquote>
<p>What if the dynamic function \(f(s,a)\) is not accurate?</p>
<blockquote>
<p>âœ… ä¸å‡†ç¡®æ¥æºäºï¼ˆ1ï¼‰æµ‹è¯•é‡è¯¯å·®ï¼ˆ2ï¼‰é—®é¢˜ç®€åŒ–</p>
</blockquote>
<p>What if the system has noise?</p>
<p>What if the system is highly nonlinear?</p>
<p>P72</p>
<h1 id="sampling-based-policy-optimization"><a class="header" href="#sampling-based-policy-optimization">Sampling-based Policy Optimization</a></h1>
<ul>
<li>
<p>Iterative methods</p>
<ul>
<li>Goal: find the optimal <strong>policy</strong> \(\pi (s;\theta )\) that minimize the objective \(J(\theta )=\sum_{t=0}^{}h(s_t,a_t) \)</li>
<li>Initialize policy parmeters \(\pi (x;\theta )\)</li>
<li>Repeat:
<ul>
<li>Propose a set of candidate parameters {\(\theta _i \)} according to \(\theta \)</li>
<li>Simulate the agent under the control of each \( \pi ( \theta _i)\) </li>
<li>Evaluate the objective function \( J (\theta_i )\)  on the simulated state-action sequences</li>
<li>Update the estimation of \(\theta \) based on {\( J (\theta_i )\)}</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Example: CMA-ES</p>
</li>
</ul>
<blockquote>
<p>âœ… åŸºäºé‡‡æ ·çš„æ–¹æ³•ã€‚</p>
</blockquote>
<p>P73</p>
<h2 id="example-locomotion-controller-with-linear-policy"><a class="header" href="#example-locomotion-controller-with-linear-policy">Example: Locomotion Controller with Linear Policy</a></h2>
<blockquote>
<p>ğŸ” [Liu et al. 2012 â€“ Terrain Runner]</p>
</blockquote>
<p>P74</p>
<h3 id="stage-1a-open-loop-policy"><a class="header" href="#stage-1a-open-loop-policy">Stage 1a: Open-loop Policy</a></h3>
<p>Find open-loop control using SAMCON</p>
<p><img src="./assets/12-26.png" alt="" /> </p>
<blockquote>
<p>âœ… ä½¿ç”¨å¼€ç¯è½¨è¿¹ä¼˜åŒ–å¾—åˆ°å¼€ç¯æ§åˆ¶è½¨è¿¹ã€‚</p>
</blockquote>
<p>P76</p>
<h3 id="stage-1b-linear-feedback-policy"><a class="header" href="#stage-1b-linear-feedback-policy">Stage 1b: Linear Feedback Policy</a></h3>
<p><img src="./assets/12-27.png" alt="" /></p>
<blockquote>
<p>âœ… ä½¿ç”¨åé¦ˆæ§åˆ¶æ›´æ–°æ§åˆ¶ä¿¡å·ã€‚ç”±äºå‡è®¾äº†çº¿æ€§å…³ç³»ï¼Œæ ¹æ®åç¦» offset å¯ç›´æ¥å¾—åˆ°è°ƒæ•´ offset.</p>
</blockquote>
<p>P78</p>
<h3 id="stage-1b-reduced-order-closed-loop-policy"><a class="header" href="#stage-1b-reduced-order-closed-loop-policy">Stage 1b: Reduced-order Closed-loop Policy</a></h3>
<p><img src="./assets/12-28.png" alt="" /></p>
<blockquote>
<p>âœ… æŠŠ \(M\) åˆ†è§£ä¸ºä¸¤ä¸ªçŸ©é˜µï¼Œ\(M_{AXB} = M_{AXC}\cdot M_{CXB}\) å¦‚æœ \(C\) æ¯”è¾ƒå°ï¼Œå¯ä»¥æ˜æ˜¾å‡å°‘çŸ©é˜µçš„å‚æ•°é‡ã€‚<br />
âœ… å¥½å¤„ï¼š(1) å‡å°‘å‚æ•°ï¼Œå‡åŒ–ä¼˜åŒ–è¿‡ç¨‹ã€‚(2) æŠ¹æ‰çŠ¶æ€é‡Œä¸éœ€è¦çš„ä¿¡æ¯ã€‚</p>
</blockquote>
<p>P79</p>
<h4 id="manually-selected-states-s"><a class="header" href="#manually-selected-states-s">Manually-selected States: s</a></h4>
<ul>
<li>Running: 12 dimensions</li>
</ul>
<p><img src="./assets/12-29.png" alt="" /> </p>
<blockquote>
<p>âœ… ï¼ˆ1ï¼‰æ ¹ç»“ç‚¹æ—‹è½¬ï¼ˆ2ï¼‰è´¨å¿ƒä½ç½®ï¼ˆ3ï¼‰è´¨å¿ƒé€Ÿåº¦ï¼ˆ4ï¼‰æ”¯æ’‘è„šä½ç½®</p>
</blockquote>
<p>P80</p>
<h4 id="manually-selected-controls-a"><a class="header" href="#manually-selected-controls-a">Manually-selected Controls: a</a></h4>
<ul>
<li>for all skills: 9 dimensions</li>
</ul>
<p><img src="./assets/12-30.png" alt="" /></p>
<blockquote>
<p>âœ… ä»…å¯¹å°‘æ•°å…³èŠ‚åŠ åé¦ˆã€‚</p>
</blockquote>
<p>P81</p>
<h3 id="optimization"><a class="header" href="#optimization">Optimization</a></h3>
<p>$$
\delta a=M\delta s+\hat{a} 
$$</p>
<ul>
<li>Optimize \(M\)
<ul>
<li>CMA, Covariance Matrix Adaption ([Hansen 2006])</li>
<li>For the running task:
<ul>
<li>#optimization variables: \(12 ^\ast 9 = 108 / (12^\ast 3+3 ^\ast 9) = 63\)</li>
</ul>
</li>
<li>12 minutes on 24 cores</li>
</ul>
</li>
</ul>
<p>P85</p>
<h1 id="optimal-control-leftrightarrow--reinforcement-learning"><a class="header" href="#optimal-control-leftrightarrow--reinforcement-learning">Optimal Control \(\Leftrightarrow \) Reinforcement Learning</a></h1>
<p>â€¢ RL shares roughly the same overall goal with Optimal Control</p>
<p>$$
\max \sum_{t=0}^{} r (s_t,a_t)
$$</p>
<blockquote>
<p>âœ… ç›¸åŒç‚¹ï¼šç›®æ ‡å‡½æ•°ç›¸åŒï¼Œæ˜¯æ¯ä¸€æ—¶åˆ»çš„ä»£ä»·å‡½æ•°ä¹‹å’Œã€‚ </p>
</blockquote>
<p>â€¢ But RL typically does not assume perfect knowledge of system</p>
<p><img src="./assets/12-30-1.png" alt="" /> </p>
<blockquote>
<p>âœ… æœ€ä¼˜æ§åˆ¶è¦æ±‚æœ‰ç²¾ç¡®çš„è¿åŠ¨æ–¹ç¨‹ï¼Œè€Œ RL ä¸éœ€è¦ã€‚</p>
</blockquote>
<ul>
<li>RL can still take advantage of a system model â†’ model-based RL
<ul>
<li>The model can be learned from data<br />
$$
s_{t+1}=f(s_t,a_t;\theta )
$$</li>
</ul>
</li>
</ul>
<blockquote>
<p>âœ… RL é€šè¿‡ä¸æ–­ä¸ä¸–ç•Œäº¤äº’è¿›è¡Œé‡‡æ ·ã€‚</p>
</blockquote>
<p>P87</p>
<h2 id="markov-decision-process-mdp"><a class="header" href="#markov-decision-process-mdp">Markov Decision Process (MDP)</a></h2>
<p><img src="./assets/12-32.png" alt="" /></p>
<p><img src="./assets/12-31.png" alt="" /></p>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td>State</td><td>\(\quad s_t \quad \quad \)</td></tr>
<tr><td>Action</td><td>\(\quad a_t\)</td></tr>
<tr><td>Policy</td><td>\(\quad \quad a_t\sim \pi (\cdot \mid s_t)\)</td></tr>
<tr><td>Transition probability</td><td>\(\quad \quad s_{t+1}\sim p  (\cdot \mid s_t,a_t)\)</td></tr>
<tr><td>Reward</td><td>\(\quad \quad r_t=r (s_t,a_t)\)</td></tr>
<tr><td>Return</td><td>\(R = \sum _{t}^{} \gamma ^t r (s_t,a_t)\)</td></tr>
</tbody></table>
<blockquote>
<p>âœ… çœŸå®åœºæ™¯ä¸­è½¨è¿¹æ— é™é•¿ï¼Œä¼šå¯¼åˆ° \(R\) æ— é™å¤§ã€‚<br />
âœ… å› æ­¤ä¼šä½¿ç”¨å°äº 1 çš„ \(r,t\) è¶Šå¤§åˆ™å¯¹ç»“æœçš„å½±å“è¶Šå°ã€‚</p>
</blockquote>
<p>P88</p>
<h2 id="è·Ÿè¸ªé—®é¢˜å˜æˆmdpé—®é¢˜"><a class="header" href="#è·Ÿè¸ªé—®é¢˜å˜æˆmdpé—®é¢˜">è·Ÿè¸ªé—®é¢˜å˜æˆMDPé—®é¢˜</a></h2>
<p>Trajectory</p>
<p>$$
\begin{matrix}
\tau =&amp; s_0 &amp; a_0 &amp; s_1 &amp; a_1 &amp; s_2&amp;\dots 
\end{matrix}
$$</p>
<p>Reward</p>
<p><img src="./assets/12-33.png" alt="" /></p>
<p>P90</p>
<h2 id="mdpé—®é¢˜çš„æ•°å­¦æè¿°"><a class="header" href="#mdpé—®é¢˜çš„æ•°å­¦æè¿°">MDPé—®é¢˜çš„æ•°å­¦æè¿°</a></h2>
<blockquote>
<p>âœ… Markov æ€§è´¨ï¼šå½“å½“å‰çŠ¶æ€å·²çŸ¥çš„æƒ…å†µä¸‹ï¼Œä¸‹ä¸€æ—¶åˆ»çŠ¶æ€åªä¸å½“å‰çŠ¶æ€ç›¸å…³ï¼Œè€Œä¸ä¸ä¹‹å‰ä»»ä¸€æ—¶åˆ»çŠ¶æ€ç›¸å…³ã€‚</p>
</blockquote>
<p>MDP is a <strong>discrete-time</strong> stochastic control process.<br />
It provides a mathematical framework for modeling decision making in situations<br />
where outcomes are <strong>partly random and partly under the control of a decision maker</strong>.</p>
<p>A MDP problem:</p>
<p>\(\mathcal{M}\) = {\(S,A,p,r\)}<br />
\(S\): state space<br />
\(A\): action space<br />
pï¼šçŠ¶æ€è½¬ç§»æ¦‚ç‡ï¼Œå³è¿åŠ¨å­¦æ–¹ç¨‹ã€‚<br />
rï¼šä»£ä»·å‡½æ•°ã€‚</p>
<p>P91</p>
<p>Solve for a policy \(\pi (a\mid s)\) that optimize the <strong>expected return</strong></p>
<p>$$
J=E[R]=E_{\tau \sim \pi }[\sum_{t}^{} \gamma ^tr(s_t,a_t)]
$$</p>
<blockquote>
<p>âœ… æ±‚è§£ä¸€ä¸ªpolicy \(\pi \) ä½¿æœŸæœ›æœ€ä¼˜ï¼Œè€Œä¸æ˜¯ç›´æ¥æ‰¾æœ€ä¼˜è§£ã€‚</p>
</blockquote>
<p>Overall all trajectories \(\tau \) = { \(s_0, a_0 , s_1 , a_1 ,  \dots  \)} induced by \(\pi \)</p>
<blockquote>
<p>âœ… å‡è®¾ \(\pi \) å‡½æ•°å’Œ \(p\) å‡½æ•°éƒ½æ˜¯æœ‰å™ªéŸ³çš„ï¼Œå³å¾—åˆ°çš„ç»“æœä¸æ˜¯ç¡®å®šå€¼ï¼Œè€Œæ˜¯ä»¥ä¸€å®šæ¦‚ç‡å¾—åˆ°æŸä¸ªç»“æœã€‚<strong>è¿™æ˜¯ä¸æœ€ä¼˜æ§åˆ¶é—®é¢˜çš„åŒºåˆ«ã€‚</strong></p>
</blockquote>
<p>P93</p>
<h2 id="bellman-equations"><a class="header" href="#bellman-equations">Bellman Equations</a></h2>
<p>In optimal control:</p>
<p><img src="./assets/12-34.png" alt="" /></p>
<p>In RL control:</p>
<p><img src="./assets/12-35.png" alt="" /></p>
<blockquote>
<p>âœ… æ­¤å¤„çš„\(\pi \)æ˜¯æŸä¸€ä¸ªç­–ç•¥ï¼Œè€Œä¸æ˜¯æœ€ä¼˜ç­–ç•¥ã€‚</p>
</blockquote>
<p>P94</p>
<h2 id="how-to-solve-mdp"><a class="header" href="#how-to-solve-mdp">How to Solve MDP</a></h2>
<h3 id="value-based-methods"><a class="header" href="#value-based-methods">Value-based Methods</a></h3>
<ul>
<li>Learning the value function/Q-function using the Bellman equations</li>
<li>Evaluation the policy as</li>
</ul>
<p>$$
\pi (s) = \arg \min_{a} Q(s,a)
$$</p>
<ul>
<li>Typically used for <strong>discrete</strong> problems</li>
<li>Example: Value iteration, Q-l a ning, DQN, â€¦</li>
</ul>
<p>P95</p>
<blockquote>
<p>ğŸ” DQN [Mnih et al. 2015, Human-level control through deep reinforcement learning]</p>
</blockquote>
<p>P96</p>
<h3 id="ç›¸å…³å·¥ä½œ"><a class="header" href="#ç›¸å…³å·¥ä½œ">ç›¸å…³å·¥ä½œ</a></h3>
<blockquote>
<p>ğŸ” [Liu et al. 2017: Learning to Schedule Control Fragments ]</p>
</blockquote>
<p><img src="./assets/12-36.png" alt="" /></p>
<blockquote>
<p>âœ… DQN æ–¹æ³•è¦æ±‚æ§åˆ¶ç©ºé—´å¿…é¡»æ˜¯ç¦»æ•£çš„ï¼Œä½†çŠ¶æ€ç©ºé—´å¯ä»¥æ˜¯è¿ç»­çš„ã€‚<br />
âœ… å› æ­¤å¯ç”¨äºé«˜é˜¶çš„æ§åˆ¶ã€‚</p>
</blockquote>
<p>P97</p>
<h3 id="policy-gradient-approach"><a class="header" href="#policy-gradient-approach">Policy Gradient approach</a></h3>
<ul>
<li>
<p>Learning the value function/Q-function using the Bellman equations</p>
</li>
<li>
<p>Compute approximate <strong>policy gradient</strong> according to value functions using Monte-Carlo method</p>
</li>
<li>
<p>Update the policy using policy gradient</p>
</li>
<li>
<p>Suitable for <strong>continuous</strong> problems</p>
</li>
<li>
<p>Exa pl : REINFORCE, TRPO, PPO, â€¦</p>
</li>
</ul>
<blockquote>
<p>âœ… policy grodient æ˜¯ Value function å¯¹çŠ¶æ€å‚æ•°çš„æ±‚å¯¼ã€‚ä½†è¿™ä¸ªæ²¡æ³•ç®—ï¼Œæ‰€ä»¥ç”¨ç»Ÿè®¡çš„æ–¹æ³•å¾—åˆ°è¿‘ä¼¼ã€‚<br />
âœ… ç‰¹ç‚¹æ˜¯æ˜¾ç¤ºå®šä¹‰ Dolicy å‡½æ•°ã€‚å¯¹è¿ç»­é—®é¢˜æ›´æœ‰æ•ˆã€‚</p>
</blockquote>
<p>P98</p>
<h3 id="ç›¸å…³å·¥ä½œ-1"><a class="header" href="#ç›¸å…³å·¥ä½œ-1">ç›¸å…³å·¥ä½œ</a></h3>
<table><thead><tr><th></th><th></th><th></th></tr></thead><tbody>
<tr><td><img src="./assets/12-37.png" alt="" /></td><td><img src="./assets/12-38.png" alt="" /></td><td><img src="./assets/12-39.png" alt="" /></td></tr>
<tr><td>[Liu et al. 2016. ControlGraphs]</td><td>[Liu et al. 2018]</td><td>[Peng et al. 2018. DeepMimic]</td></tr>
</tbody></table>
<p>P100</p>
<h2 id="generative-control-policies"><a class="header" href="#generative-control-policies">Generative Control Policies</a></h2>
<blockquote>
<p>âœ… ä½¿ç”¨RL learningï¼ŒåŠ ä¸Šä¸€ç‚¹ç‚¹è½¨è¿¹ä¼˜åŒ–çš„æ§åˆ¶ï¼Œå°±å¯ä»¥å®ç°éå¸¸å¤æ‚çš„åŠ¨ä½œã€‚</p>
</blockquote>
<blockquote>
<p>ğŸ” [Yao et al. Control VAE]</p>
</blockquote>
<p>P101</p>
<h1 id="whats-next"><a class="header" href="#whats-next">Whatâ€™s Next?</a></h1>
<h2 id="digital-cerebellum"><a class="header" href="#digital-cerebellum">Digital Cerebellum</a></h2>
<p>Large Pretrained Model for Motion Control</p>
<p><img src="./assets/12-40.png" alt="" /></p>
<p>P102</p>
<h2 id="cross-modality-generation"><a class="header" href="#cross-modality-generation">Cross-modality Generation</a></h2>
<ul>
<li>\(\Leftrightarrow\) LLM \(\Leftrightarrow\) Text/Audio \(\Leftrightarrow\) Motion/Control \(\Leftrightarrow\) Image/Video \(\Leftrightarrow\)</li>
<li>Digital Actor?</li>
</ul>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/GAMES105_mdbook/</p>
</blockquote>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="Learning.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="Learning.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="theme/pagetoc.js"></script>
    </body>
</html>
