<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Optimal Control and Reinforcement Learning - GAMES105</title>
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="The note of GAMES105">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/pagetoc.css">
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><div>GAMES105</div></li><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="Introduction.html"><strong aria-hidden="true">2.</strong> Introduction to 3D Character Animation</a></li><li class="chapter-item expanded "><a href="Math.html"><strong aria-hidden="true">3.</strong> Math Background</a></li><li class="chapter-item expanded "><a href="Forward.html"><strong aria-hidden="true">4.</strong> Character Kinematics:Forward and Inverse Kinematics</a></li><li class="chapter-item expanded "><a href="Keyframe.html"><strong aria-hidden="true">5.</strong> Character Kinematics (cont.) &amp; Keyframe Animation</a></li><li class="chapter-item expanded "><a href="Data-driven.html"><strong aria-hidden="true">6.</strong> Data-driven Character Animation</a></li><li class="chapter-item expanded "><a href="Learning-based.html"><strong aria-hidden="true">7.</strong> Statistical Models of Human Motion</a></li><li class="chapter-item expanded "><a href="cont.html"><strong aria-hidden="true">8.</strong> Learning-based Character Animation</a></li><li class="chapter-item expanded "><a href="Skinning.html"><strong aria-hidden="true">9.</strong> Skinning</a></li><li class="chapter-item expanded "><a href="Simulation.html"><strong aria-hidden="true">10.</strong> Physics-based Simulation and Articulated Rigid Bodies</a></li><li class="chapter-item expanded "><a href="Actuating.html"><strong aria-hidden="true">11.</strong> Actuating Simulated Characters</a></li><li class="chapter-item expanded "><a href="Controlling.html"><strong aria-hidden="true">12.</strong> Controlling Characters</a></li><li class="chapter-item expanded "><a href="Learning.html"><strong aria-hidden="true">13.</strong> Learning to Walk</a></li><li class="chapter-item expanded "><a href="Optimal.html" class="active"><strong aria-hidden="true">14.</strong> Optimal Control and Reinforcement Learning</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">GAMES105</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/CaterpillarStudyGroup/GAMES105_mdbook" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main><div class="sidetoc"><nav class="pagetoc"></nav></div>
                        <p>P2</p>
<h1 id="outline"><a class="header" href="#outline">Outline</a></h1>
<ul>
<li>
<p>Optimal Control</p>
</li>
<li>
<p>Model-based Approaches vs. Model-free Approaches</p>
</li>
<li>
<p>Sampling-based Optimization</p>
</li>
<li>
<p>Reinforcement Learning</p>
</li>
<li>
<p>Conclusion</p>
</li>
</ul>
<p>P3</p>
<h2 id="recap"><a class="header" href="#recap">Recap</a></h2>
<table><thead><tr><th>feedforward</th><th>feedback</th></tr></thead><tbody>
<tr><td><img src="./assets/12-01.png" alt="" /></td><td><img src="./assets/12-04.png" alt="" /></td></tr>
<tr><td><img src="./assets/12-02.png" alt="" /></td><td><img src="./assets/12-03.png" alt="" /></td></tr>
</tbody></table>
<blockquote>
<p>✅ 开环控制：只考虑初始状态。<br />
✅ 前馈控制：考虑初始状态和干挠。<br />
✅ 前馈控制优化的是轨迹。<br />
✅ 反馈控制优化的是控制策略，控制策略是一个函数，根据当前状态优化轨迹。</p>
</blockquote>
<p>P9</p>
<p><img src="./assets/12-05.png" alt="" /></p>
<blockquote>
<p>✅ Feedback 类似构造一个场，把任何状态推到目标状态。</p>
</blockquote>
<p>P10</p>
<h1 id="开环控制"><a class="header" href="#开环控制">开环控制</a></h1>
<h2 id="问题描述"><a class="header" href="#问题描述">问题描述</a></h2>
<p>$$
\begin{matrix}
\min_{x}  f(x)\\
𝑠.𝑡. g(x)=0
\end{matrix}
$$</p>
<p><img src="./assets/12-06.png" alt="" /></p>
<p>P12</p>
<h2 id="把硬约束转化为软约束"><a class="header" href="#把硬约束转化为软约束">把硬约束转化为软约束</a></h2>
<p>$$
\min_{x}  f(x)+ wg(x)
$$</p>
<p>\(^\ast \) The solution \(x^\ast\)  may not satisfy the constraint</p>
<p>P16</p>
<h2 id="lagrange-multiplier---把约束条件转化为优化"><a class="header" href="#lagrange-multiplier---把约束条件转化为优化">Lagrange Multiplier - 把约束条件转化为优化</a></h2>
<blockquote>
<p>✅ 拉格朗日乘子法。</p>
</blockquote>
<p><img src="./assets/12-08.png" alt="" /></p>
<blockquote>
<p>✅ 通过观察可知，极值点位于\({f}'(x)\) 与 \(g\) 的切线垂直，即 \({f}' (x)\) 与 \({g}' (x)\) 平行。（充分非必要条件。）</p>
</blockquote>
<p>因此：</p>
<p><img src="./assets/12-07.png" alt="" /></p>
<p>Lagrange function</p>
<p>$$
L(x,\lambda )=f(x)+\lambda ^Tg(x)
$$</p>
<blockquote>
<p>✅ 把约束条件转化为优化。</p>
</blockquote>
<p>P18</p>
<h2 id="lagrange-multiplier"><a class="header" href="#lagrange-multiplier">Lagrange Multiplier</a></h2>
<p><img src="./assets/12-09-1.png" alt="" /></p>
<blockquote>
<p>✅ 这是一个优化问题，通过梯度下降找到极值点。</p>
</blockquote>
<p>P20</p>
<h2 id="solving-trajectory-optimization-problem"><a class="header" href="#solving-trajectory-optimization-problem">Solving Trajectory Optimization Problem</a></h2>
<h3 id="定义带约束的优化问题"><a class="header" href="#定义带约束的优化问题">定义带约束的优化问题</a></h3>
<p>Find a control sequence {\(a_t\)} that generates a state sequence {\(s_t\)} start from \(s_o\) minimizes</p>
<p>$$
\min h (s_r)+\sum _{t=0}^{T-1} h(s_t,a_t)
$$</p>
<blockquote>
<p>✅ 因为把时间离散化，此处用求和不用积分。</p>
</blockquote>
<p>subject to</p>
<p>$$
\begin{matrix}
f(s_t,a_t)-s_{t+1}=0\\
\text{ for } 0 \le t &lt; T
\end{matrix}
$$</p>
<blockquote>
<p>✅ 运动学方程，作为约束</p>
</blockquote>
<h3 id="转化为优化问题"><a class="header" href="#转化为优化问题">转化为优化问题</a></h3>
<p>The Lagrange function</p>
<p>$$
L(s,a,\lambda ) = h(s _ T)+ \sum _ {t=0} ^ {T-1} h(s _t,a _t) + \lambda _ {t+1}^T(f(s _t,a _t) - s _ {t+1})
$$</p>
<p>P27</p>
<h3 id="求解拉格朗日方程"><a class="header" href="#求解拉格朗日方程">求解拉格朗日方程</a></h3>
<p><img src="./assets/12-10-1.png" alt="" /></p>
<blockquote>
<p>✅ 拉格朗日方程，对每个变量求导，并令导数为零。因此得到右边方程组。<br />
✅ 右边方程组进一步整理，得到左边。<br />
✅ \(\lambda \) 类似于逆向仿真。<br />
✅ 公式 3：通过转为优化问题求 \(a\)．</p>
</blockquote>
<p>P30</p>
<h3 id="pontryagins-maximum-principle-for-discrete-systems"><a class="header" href="#pontryagins-maximum-principle-for-discrete-systems">Pontryagin’s Maximum Principle for discrete systems</a></h3>
<p><img src="./assets/12-11.png" alt="" /></p>
<p><img src="./assets/12-12.png" alt="" /></p>
<blockquote>
<p>✅ 方程组整理得到左边，称为 PMP 条件。是开环控制最优的必要条件。</p>
</blockquote>
<p>P32</p>
<h2 id="optimal-control"><a class="header" href="#optimal-control">Optimal Control</a></h2>
<p><strong>Open-loop Control</strong>:<br />
given a start state \(s_0\), compute sequence of actions {\(a_t\)} to reach the goal</p>
<p><img src="./assets/12-13.png" alt="" /></p>
<blockquote>
<p><strong>Shooting method</strong> directly applies PMP. However, it does not scale well to complicated problems such as motion control…<br />
\(<br>\)<br />
Need to be combined with collocation method, multiple shooting, etc. for those problems.<br />
\(<br>\)<br />
Or use derivative-free approaches.</p>
</blockquote>
<p><img src="./assets/12-14.png" alt="" /></p>
<blockquote>
<p>✅ 对于复杂函数，表现比较差，还需要借助其它方法。</p>
</blockquote>
<h1 id="闭环控制"><a class="header" href="#闭环控制">闭环控制</a></h1>
<p><img src="./assets/12-05.png" alt="" /></p>
<p>P34</p>
<h2 id="dynamic-programming"><a class="header" href="#dynamic-programming">Dynamic Programming</a></h2>
<p><img src="./assets/12-15.png" alt="" /> </p>
<p>希望找到一条最短路径到达另一个点，对这个问题用不同的方式建模，会得到不同的方法：</p>
<table><thead><tr><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>动态规划问题</td><td>Find a path {\(s_t\)} that minimizes</td><td>\(J(s_0)=\sum _ {t=0}^{ } h(s_t,s_{t+1})\)</td></tr>
<tr><td>轨迹问题</td><td>Find a sequence of action {\(a_t\)} that minimizes</td><td>\(J(s_0)=\sum _ {t=0}^{ } h(s_t,a_t)\)<br> subject to <br> \( s_{t+1}=f(s_t,a_t)\)</td></tr>
<tr><td>控制策略问题</td><td>Find a policy \( a_t=\pi (s_t,t)\)或 \( a_t=\pi (s_t)\)that minimizes</td><td>\(J(s_0)=\sum _ {t=0}^{ } h(s_t,a_t)\)<br>subject to  <br>\(s_{t+1}=f(s_t,a_t)\)</td></tr>
</tbody></table>
<p>P39</p>
<h2 id="bellmans-principle-of-optimality"><a class="header" href="#bellmans-principle-of-optimality">Bellman’s Principle of Optimality</a></h2>
<blockquote>
<p>✅ 针对控制策略问题，什么样的策略是最优策略？</p>
</blockquote>
<p><img src="./assets/12-16.png" alt="" /> </p>
<p>An optimal policy has the property that whatever the initial 
state and initial decision are, the remaining decisions must 
constitute an optimal policy with regard to the state resulting 
from the first decision.</p>
<p>\(^\ast \) The problem is said to have <strong>optimal substructure</strong></p>
<p>P40</p>
<h2 id="value-function"><a class="header" href="#value-function">Value Function</a></h2>
<p>Value of a state \(V(s)\) :</p>
<ul>
<li>the minimal total cost for finishing the task starting from \(s\)</li>
<li>the total cost for finishing the task starting from \(s\) using the optimal policy</li>
</ul>
<blockquote>
<p>✅ Value Funcron，计算从某个结点到 gool 的最小代价。<br />
✅ 后面动态规划原理跳过。</p>
</blockquote>
<p>P49</p>
<h2 id="the-bellman-equation"><a class="header" href="#the-bellman-equation">The Bellman Equation</a></h2>
<p>Mathematically, an optimal <strong>value function</strong> \(V(s)\) can be defined recursively as:</p>
<p>$$
V(s)=\min_{a} (h(s,a)+V(f(s,a)))
$$</p>
<blockquote>
<p>✅ h代表s状态下执行一步a的代价。f代表以s状态下执行一步a之后的状态。</p>
</blockquote>
<p>If we know this value function, the optimal <strong>policy</strong> can be computed as</p>
<p>$$
\pi (s)=\arg \min_{a} (h(s,a)+V(f(s,a)))
$$</p>
<blockquote>
<p>✅ pi代表一种策略，根据当前状态s找到最优的下一步a。<br />
✅ This arg max can be easily computed for discrete control problems.<br />
But there are not always closed-forms solution for continuous control problems.</p>
</blockquote>
<p>or</p>
<p>$$
\begin{matrix}
\pi (s)=\arg \min_{a} Q(s,a)\\
\text{where} \quad \quad  Q(s,a)=h(s,a)+V(f(s,a))
\end{matrix}
$$</p>
<p>Q-function称为State-action value function<br />
Learning \(V(s)\) and/or \(Q(s,a)\) is the core of optimal control / reinforcement learning methods</p>
<blockquote>
<p>✅ 强化学习最主要的目的是学习 \(V\) 函数和 \(Q\) 函数，如果 \(a\) 是有限状态，遍历即可。但在角色动画里，\(a\) 是连续状态。</p>
</blockquote>
<p>P52</p>
<h1 id="linear-quadratic-regulator-lqr"><a class="header" href="#linear-quadratic-regulator-lqr">Linear Quadratic Regulator (LQR)</a></h1>
<p><img src="./assets/12-17.png" alt="" /> </p>
<ul>
<li>LQR is a special class of optimal control problems with
<ul>
<li><strong>Linear</strong> dynamic function</li>
<li><strong>Quadratic</strong> objective function</li>
</ul>
</li>
</ul>
<blockquote>
<p>✅ LQR 是控制领域一类经典问题，它对原控制问题做了一些特定的约束。因为简化了问题，可以得到有特定公式的 \(Q\) 和 \(V\).</p>
</blockquote>
<p>P53</p>
<h2 id="a-very-simple-example"><a class="header" href="#a-very-simple-example">A very simple example</a></h2>
<h3 id="问题描述-1"><a class="header" href="#问题描述-1">问题描述</a></h3>
<p><img src="./assets/12-18.png" alt="" /> </p>
<p>Compute a target trajectory \(\tilde{x}(t)\) such that the simulated trajectory \(x(t)\) is a sine curve.</p>
<p><img src="./assets/12-19.png" alt="" /> </p>
<p>$$
\min _{(x_n,v_n,\tilde{x} _n)} \sum _{n=0}^{N} (\sin (t_n)-x_n)^2+\sum _{n=0}^{N}\tilde{x}^2_n 
$$</p>
<p>$$
\begin{align*}
s.t. \quad \quad v _ {n+1} &amp; = v _ n + h(k _p ( \tilde{x} _ n - x _ n) - k _ dv _ n ) \\
v _ {x+1} &amp; = x _ n + hv _ {n+1}
\end{align*}
$$</p>
<p>P54<br />
objective function</p>
<p>$$
\min s^T_TQ_Ts_T+\sum_{t=0}^{T} s^T_tQ_ts_t+a^T_tR_ta_t
$$</p>
<p>subject to dynamic function</p>
<p>$$
s_{t+1}=A_ts_t+B_ta_t   \quad \quad \text{for }   0\le t &lt;T 
$$</p>
<blockquote>
<p>✅ 目标函数是二次函数，运动学方程是线性函数。这是一个典型的 LQR 问题。</p>
</blockquote>
<p>P58</p>
<h3 id="推导一步"><a class="header" href="#推导一步">推导一步</a></h3>
<blockquote>
<p>✅ 由于存在optimal substructure，每次只需要考虑下一个状态的最优解。<br />
✅ 每一个状态基于下一个状态来计算，不断往下迭代，直到最后一个状态。<br />
✅ 最后一个状态的V的计算与a无关。<br />
✅ 计算完最后一个，再计算倒数第二个，依次往前推。</p>
</blockquote>
<p><img src="./assets/12-20-1.png" alt="" /> </p>
<p>P60<br />
公式整理得：</p>
<p><img src="./assets/12-20.png" alt="" /> </p>
<p>P61 
<img src="./assets/12-21.png" alt="" /> </p>
<blockquote>
<p>✅ 结论：最优策略与当前状态的关系是矩阵K的关系。</p>
</blockquote>
<p>P62<br />
当a取最小值时，求出V：</p>
<p><img src="./assets/12-22.png" alt="" /></p>
<blockquote>
<p>✅ \(V(S_{T-1})\)和\(V(S_{T})\)的形式基本一致，只是P的表示不同。 </p>
</blockquote>
<p>P63</p>
<h3 id="推导每一步"><a class="header" href="#推导每一步">推导每一步</a></h3>
<p><img src="./assets/12-23.png" alt="" /> </p>
<p>P64</p>
<h3 id="solution"><a class="header" href="#solution">Solution</a></h3>
<ul>
<li>LQR is a special class of optimal control problems with
<ul>
<li>Linear dynamic function</li>
<li>Quadratic objective function</li>
</ul>
</li>
<li>Solution of LQR is a linear feedback policy</li>
</ul>
<p><img src="./assets/12-24.png" alt="" /> </p>
<p>P65</p>
<h2 id="更复杂的情况"><a class="header" href="#更复杂的情况">更复杂的情况</a></h2>
<ul>
<li>How to deal with
<ul>
<li>Nonlinear dynamic function?</li>
<li>Non-quadratic objective function?</li>
</ul>
</li>
</ul>
<blockquote>
<p>✅ 人体运动涉及到角度旋转，因此是非线性的。</p>
</blockquote>
<p>P68</p>
<h3 id="nonlinear-problems"><a class="header" href="#nonlinear-problems">Nonlinear problems</a></h3>
<p><img src="./assets/12-25.png" alt="" /> </p>
<blockquote>
<p>✅ 方法：把问题近似为线性问题。</p>
</blockquote>
<p>Approximate cost function as a quadratic function:</p>
<blockquote>
<p>✅ 目标函数：泰勒展开，保留二次。</p>
</blockquote>
<p>$$
h(s_t,a_t)\approx h(\bar{s}_t ,\bar{a}_t)+\nabla h(\bar{s}_t ,\bar{a}_t)\begin{bmatrix}
s_t-\bar{s} _t\\
a_t-\bar{a} _t
\end{bmatrix} + \frac{1}{2} \begin{bmatrix}
s_t-\bar{s} _t\\
a_t-\bar{a} _t
\end{bmatrix}^T\nabla^2h(\bar{s}_t ,\bar{a}_t)\begin{bmatrix}
s_t-\bar{s} _t\\
a_t-\bar{a} _t
\end{bmatrix}
$$</p>
<p>Approximate dynamic function as a linear function:</p>
<blockquote>
<p>✅ 转移函数：泰勒展开，保留一次或二次。</p>
</blockquote>
<p>$$
f(s_t,a_t)\approx f(\bar{s}_t ,\bar{a}_t)+\nabla f(\bar{s}_t ,\bar{a}_t)\begin{bmatrix}
s_t-\bar{s} _t\\
a_t-\bar{a} _t
\end{bmatrix}
$$</p>
<p>展开为一次项，对应解决算法：iLQR（iterative LQR） </p>
<p>Or a quadratic function:</p>
<p>$$
f(s_t,a_t)\approx \ast \ast \ast \frac{1}{2} \begin{bmatrix}
s_t-\bar{s} _t\\
a_t-\bar{a} _t
\end{bmatrix}^T\nabla^2f(\bar{s}_t ,\bar{a}_t)\begin{bmatrix}
s_t-\bar{s} _t\\
a_t-\bar{a} _t
\end{bmatrix}
$$</p>
<p>展开为二次项，对应解决算法：DDP（Differential Dynamic Programming）</p>
<p>P69</p>
<h3 id="相关应用"><a class="header" href="#相关应用">相关应用</a></h3>
<blockquote>
<p>🔎 [Muico et al 2011 - Composite Control of Physically Simulated Characters]</p>
</blockquote>
<blockquote>
<p>✅ 选择合适的 \(Q\) 和 \(R\)，需要一些工程上的技巧。<br />
✅ 为了求解方程，需要显式地建模运动学方程。</p>
</blockquote>
<p>P70</p>
<h2 id="model-based-method-vs-model-free-method"><a class="header" href="#model-based-method-vs-model-free-method">Model-based Method vs. Model-free Method</a></h2>
<blockquote>
<p>✅ Model Based 方法，要求 dynamic function 是已知的，但是实际上这个函数可能是（1）未知的（2）不精确的。<br />
✅ 因此Model Based 方法对于复杂问题难以应用，但对于简单问题非常高效。</p>
</blockquote>
<p>What if the dynamic function \(f(s,a)\) is not know?</p>
<blockquote>
<p>✅ \(f\) 未知只是把 \(f\) 当成一个黑盒子，仍需要根据 \(S_t\) 得到 \(S_{t＋1}\) .</p>
</blockquote>
<p>What if the dynamic function \(f(s,a)\) is not accurate?</p>
<blockquote>
<p>✅ 不准确来源于（1）测试量误差（2）问题简化</p>
</blockquote>
<p>What if the system has noise?</p>
<p>What if the system is highly nonlinear?</p>
<p>P72</p>
<h1 id="sampling-based-policy-optimization"><a class="header" href="#sampling-based-policy-optimization">Sampling-based Policy Optimization</a></h1>
<ul>
<li>
<p>Iterative methods</p>
<ul>
<li>Goal: find the optimal <strong>policy</strong> \(\pi (s;\theta )\) that minimize the objective \(J(\theta )=\sum_{t=0}^{}h(s_t,a_t) \)</li>
<li>Initialize policy parmeters \(\pi (x;\theta )\)</li>
<li>Repeat:
<ul>
<li>Propose a set of candidate parameters {\(\theta _i \)} according to \(\theta \)</li>
<li>Simulate the agent under the control of each \( \pi ( \theta _i)\) </li>
<li>Evaluate the objective function \( J (\theta_i )\)  on the simulated state-action sequences</li>
<li>Update the estimation of \(\theta \) based on {\( J (\theta_i )\)}</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Example: CMA-ES</p>
</li>
</ul>
<blockquote>
<p>✅ 基于采样的方法。</p>
</blockquote>
<p>P73</p>
<h2 id="example-locomotion-controller-with-linear-policy"><a class="header" href="#example-locomotion-controller-with-linear-policy">Example: Locomotion Controller with Linear Policy</a></h2>
<blockquote>
<p>🔎 [Liu et al. 2012 – Terrain Runner]</p>
</blockquote>
<p>P74</p>
<h3 id="stage-1a-open-loop-policy"><a class="header" href="#stage-1a-open-loop-policy">Stage 1a: Open-loop Policy</a></h3>
<p>Find open-loop control using SAMCON</p>
<p><img src="./assets/12-26.png" alt="" /> </p>
<blockquote>
<p>✅ 使用开环轨迹优化得到开环控制轨迹。</p>
</blockquote>
<p>P76</p>
<h3 id="stage-1b-linear-feedback-policy"><a class="header" href="#stage-1b-linear-feedback-policy">Stage 1b: Linear Feedback Policy</a></h3>
<p><img src="./assets/12-27.png" alt="" /></p>
<blockquote>
<p>✅ 使用反馈控制更新控制信号。由于假设了线性关系，根据偏离 offset 可直接得到调整 offset.</p>
</blockquote>
<p>P78</p>
<h3 id="stage-1b-reduced-order-closed-loop-policy"><a class="header" href="#stage-1b-reduced-order-closed-loop-policy">Stage 1b: Reduced-order Closed-loop Policy</a></h3>
<p><img src="./assets/12-28.png" alt="" /></p>
<blockquote>
<p>✅ 把 \(M\) 分解为两个矩阵，\(M_{AXB} = M_{AXC}\cdot M_{CXB}\) 如果 \(C\) 比较小，可以明显减少矩阵的参数量。<br />
✅ 好处：(1) 减少参数，减化优化过程。(2) 抹掉状态里不需要的信息。</p>
</blockquote>
<p>P79</p>
<h4 id="manually-selected-states-s"><a class="header" href="#manually-selected-states-s">Manually-selected States: s</a></h4>
<ul>
<li>Running: 12 dimensions</li>
</ul>
<p><img src="./assets/12-29.png" alt="" /> </p>
<blockquote>
<p>✅ （1）根结点旋转（2）质心位置（3）质心速度（4）支撑脚位置</p>
</blockquote>
<p>P80</p>
<h4 id="manually-selected-controls-a"><a class="header" href="#manually-selected-controls-a">Manually-selected Controls: a</a></h4>
<ul>
<li>for all skills: 9 dimensions</li>
</ul>
<p><img src="./assets/12-30.png" alt="" /></p>
<blockquote>
<p>✅ 仅对少数关节加反馈。</p>
</blockquote>
<p>P81</p>
<h3 id="optimization"><a class="header" href="#optimization">Optimization</a></h3>
<p>$$
\delta a=M\delta s+\hat{a} 
$$</p>
<ul>
<li>Optimize \(M\)
<ul>
<li>CMA, Covariance Matrix Adaption ([Hansen 2006])</li>
<li>For the running task:
<ul>
<li>#optimization variables: \(12 ^\ast 9 = 108 / (12^\ast 3+3 ^\ast 9) = 63\)</li>
</ul>
</li>
<li>12 minutes on 24 cores</li>
</ul>
</li>
</ul>
<p>P85</p>
<h1 id="optimal-control-leftrightarrow--reinforcement-learning"><a class="header" href="#optimal-control-leftrightarrow--reinforcement-learning">Optimal Control \(\Leftrightarrow \) Reinforcement Learning</a></h1>
<p>• RL shares roughly the same overall goal with Optimal Control</p>
<p>$$
\max \sum_{t=0}^{} r (s_t,a_t)
$$</p>
<blockquote>
<p>✅ 相同点：目标函数相同，是每一时刻的代价函数之和。 </p>
</blockquote>
<p>• But RL typically does not assume perfect knowledge of system</p>
<p><img src="./assets/12-30-1.png" alt="" /> </p>
<blockquote>
<p>✅ 最优控制要求有精确的运动方程，而 RL 不需要。</p>
</blockquote>
<ul>
<li>RL can still take advantage of a system model → model-based RL
<ul>
<li>The model can be learned from data<br />
$$
s_{t+1}=f(s_t,a_t;\theta )
$$</li>
</ul>
</li>
</ul>
<blockquote>
<p>✅ RL 通过不断与世界交互进行采样。</p>
</blockquote>
<p>P87</p>
<h2 id="markov-decision-process-mdp"><a class="header" href="#markov-decision-process-mdp">Markov Decision Process (MDP)</a></h2>
<p><img src="./assets/12-32.png" alt="" /></p>
<p><img src="./assets/12-31.png" alt="" /></p>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td>State</td><td>\(\quad s_t \quad \quad \)</td></tr>
<tr><td>Action</td><td>\(\quad a_t\)</td></tr>
<tr><td>Policy</td><td>\(\quad \quad a_t\sim \pi (\cdot \mid s_t)\)</td></tr>
<tr><td>Transition probability</td><td>\(\quad \quad s_{t+1}\sim p  (\cdot \mid s_t,a_t)\)</td></tr>
<tr><td>Reward</td><td>\(\quad \quad r_t=r (s_t,a_t)\)</td></tr>
<tr><td>Return</td><td>\(R = \sum _{t}^{} \gamma ^t r (s_t,a_t)\)</td></tr>
</tbody></table>
<blockquote>
<p>✅ 真实场景中轨迹无限长，会导到 \(R\) 无限大。<br />
✅ 因此会使用小于 1 的 \(r,t\) 越大则对结果的影响越小。</p>
</blockquote>
<p>P88</p>
<h2 id="跟踪问题变成mdp问题"><a class="header" href="#跟踪问题变成mdp问题">跟踪问题变成MDP问题</a></h2>
<p>Trajectory</p>
<p>$$
\begin{matrix}
\tau =&amp; s_0 &amp; a_0 &amp; s_1 &amp; a_1 &amp; s_2&amp;\dots 
\end{matrix}
$$</p>
<p>Reward</p>
<p><img src="./assets/12-33.png" alt="" /></p>
<p>P90</p>
<h2 id="mdp问题的数学描述"><a class="header" href="#mdp问题的数学描述">MDP问题的数学描述</a></h2>
<blockquote>
<p>✅ Markov 性质：当当前状态已知的情况下，下一时刻状态只与当前状态相关，而不与之前任一时刻状态相关。</p>
</blockquote>
<p>MDP is a <strong>discrete-time</strong> stochastic control process.<br />
It provides a mathematical framework for modeling decision making in situations<br />
where outcomes are <strong>partly random and partly under the control of a decision maker</strong>.</p>
<p>A MDP problem:</p>
<p>\(\mathcal{M}\) = {\(S,A,p,r\)}<br />
\(S\): state space<br />
\(A\): action space<br />
p：状态转移概率，即运动学方程。<br />
r：代价函数。</p>
<p>P91</p>
<p>Solve for a policy \(\pi (a\mid s)\) that optimize the <strong>expected return</strong></p>
<p>$$
J=E[R]=E_{\tau \sim \pi }[\sum_{t}^{} \gamma ^tr(s_t,a_t)]
$$</p>
<blockquote>
<p>✅ 求解一个policy \(\pi \) 使期望最优，而不是直接找最优解。</p>
</blockquote>
<p>Overall all trajectories \(\tau \) = { \(s_0, a_0 , s_1 , a_1 ,  \dots  \)} induced by \(\pi \)</p>
<blockquote>
<p>✅ 假设 \(\pi \) 函数和 \(p\) 函数都是有噪音的，即得到的结果不是确定值，而是以一定概率得到某个结果。<strong>这是与最优控制问题的区别。</strong></p>
</blockquote>
<p>P93</p>
<h2 id="bellman-equations"><a class="header" href="#bellman-equations">Bellman Equations</a></h2>
<p>In optimal control:</p>
<p><img src="./assets/12-34.png" alt="" /></p>
<p>In RL control:</p>
<p><img src="./assets/12-35.png" alt="" /></p>
<blockquote>
<p>✅ 此处的\(\pi \)是某一个策略，而不是最优策略。</p>
</blockquote>
<p>P94</p>
<h2 id="how-to-solve-mdp"><a class="header" href="#how-to-solve-mdp">How to Solve MDP</a></h2>
<h3 id="value-based-methods"><a class="header" href="#value-based-methods">Value-based Methods</a></h3>
<ul>
<li>Learning the value function/Q-function using the Bellman equations</li>
<li>Evaluation the policy as</li>
</ul>
<p>$$
\pi (s) = \arg \min_{a} Q(s,a)
$$</p>
<ul>
<li>Typically used for <strong>discrete</strong> problems</li>
<li>Example: Value iteration, Q-l a ning, DQN, …</li>
</ul>
<p>P95</p>
<blockquote>
<p>🔎 DQN [Mnih et al. 2015, Human-level control through deep reinforcement learning]</p>
</blockquote>
<p>P96</p>
<h3 id="相关工作"><a class="header" href="#相关工作">相关工作</a></h3>
<blockquote>
<p>🔎 [Liu et al. 2017: Learning to Schedule Control Fragments ]</p>
</blockquote>
<p><img src="./assets/12-36.png" alt="" /></p>
<blockquote>
<p>✅ DQN 方法要求控制空间必须是离散的，但状态空间可以是连续的。<br />
✅ 因此可用于高阶的控制。</p>
</blockquote>
<p>P97</p>
<h3 id="policy-gradient-approach"><a class="header" href="#policy-gradient-approach">Policy Gradient approach</a></h3>
<ul>
<li>
<p>Learning the value function/Q-function using the Bellman equations</p>
</li>
<li>
<p>Compute approximate <strong>policy gradient</strong> according to value functions using Monte-Carlo method</p>
</li>
<li>
<p>Update the policy using policy gradient</p>
</li>
<li>
<p>Suitable for <strong>continuous</strong> problems</p>
</li>
<li>
<p>Exa pl : REINFORCE, TRPO, PPO, …</p>
</li>
</ul>
<blockquote>
<p>✅ policy grodient 是 Value function 对状态参数的求导。但这个没法算，所以用统计的方法得到近似。<br />
✅ 特点是显示定义 Dolicy 函数。对连续问题更有效。</p>
</blockquote>
<p>P98</p>
<h3 id="相关工作-1"><a class="header" href="#相关工作-1">相关工作</a></h3>
<table><thead><tr><th></th><th></th><th></th></tr></thead><tbody>
<tr><td><img src="./assets/12-37.png" alt="" /></td><td><img src="./assets/12-38.png" alt="" /></td><td><img src="./assets/12-39.png" alt="" /></td></tr>
<tr><td>[Liu et al. 2016. ControlGraphs]</td><td>[Liu et al. 2018]</td><td>[Peng et al. 2018. DeepMimic]</td></tr>
</tbody></table>
<p>P100</p>
<h2 id="generative-control-policies"><a class="header" href="#generative-control-policies">Generative Control Policies</a></h2>
<blockquote>
<p>✅ 使用RL learning，加上一点点轨迹优化的控制，就可以实现非常复杂的动作。</p>
</blockquote>
<blockquote>
<p>🔎 [Yao et al. Control VAE]</p>
</blockquote>
<p>P101</p>
<h1 id="whats-next"><a class="header" href="#whats-next">What’s Next?</a></h1>
<h2 id="digital-cerebellum"><a class="header" href="#digital-cerebellum">Digital Cerebellum</a></h2>
<p>Large Pretrained Model for Motion Control</p>
<p><img src="./assets/12-40.png" alt="" /></p>
<p>P102</p>
<h2 id="cross-modality-generation"><a class="header" href="#cross-modality-generation">Cross-modality Generation</a></h2>
<ul>
<li>\(\Leftrightarrow\) LLM \(\Leftrightarrow\) Text/Audio \(\Leftrightarrow\) Motion/Control \(\Leftrightarrow\) Image/Video \(\Leftrightarrow\)</li>
<li>Digital Actor?</li>
</ul>
<hr />
<blockquote>
<p>本文出自CaterpillarStudyGroup，转载请注明出处。</p>
<p>https://caterpillarstudygroup.github.io/GAMES105_mdbook/</p>
</blockquote>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="Learning.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="Learning.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="theme/pagetoc.js"></script>
    </body>
</html>
